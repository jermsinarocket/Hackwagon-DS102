{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i2.wp.com/hackwagon.com/wp-content/uploads/2017/02/Logo-Web-Export.png?ssl=1\" width=200/></center>\n",
    "<h1> Hackwagon Academy DS102 Lesson 4B </h1>\n",
    "<h2> Text Normalisation</h2> \n",
    "<h3> Lesson Outline </h3>\n",
    "\n",
    "- 1. [Text Analysis](#1)\n",
    "- 2. [Terminologies](#2)\n",
    "- 3. [Text Normalisation](#3)\n",
    "- 4. [Simple Cleaning](#4)\n",
    "    - 4.1 [Lowercase](#4.1)\n",
    "    - 4.2 [Strip Spaces](#4.2)\n",
    "    - 4.3 [Regex Cleaning](#4.3)\n",
    "    - [Practice I](#P1)\n",
    "- 5. [Tokenisation](#5)\n",
    "- 6. [Method 1 - Lemmatisation](#6)\n",
    "    - 6.1 [POS Tagging](#6.1)\n",
    "    - 6.2 [Lemmatisation](#6.2)\n",
    "- 7. [Method 2 - Stemming](#7)\n",
    "- 8. [Stemming vs. Lemming](#8)\n",
    "- 9. [Stop Word Removal](#9)\n",
    "- [Practice II](#P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "<a id='1'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;1.</font><font color=\"salmon\"> Text Analysis </font> </h2></a>\n",
    "\n",
    "![overview](https://i.imgur.com/pqrH1r3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "NLP is Natural Language Processing, finding useful insights from unstructured textual data. \n",
    "\n",
    "**Practical**\n",
    "\n",
    "* Spam Detection\n",
    "* Censorship / Filtering Sexually Explicit Content\n",
    "* Organizing / Categorizing Documents\n",
    "* Determining people's impression of your company\n",
    "\n",
    "**Theoretical**\n",
    "\n",
    "* Understanding intent of words (question answering)\n",
    "* Understanding references in sentences (coreference resolution)\n",
    "    * The man tried to pick up his son but he was weak. Who does \"he\" refer to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;2.</font><font color=\"salmon\"> Terminologies </font> </h2></a>\n",
    "\n",
    "In Text Mining, we often use the following terms to refer to collections of words\n",
    "\n",
    "- <b>Corpus</b>\n",
    "    - Large collection of texts, represented by documents\n",
    "    - Corpora is a collection of corpus\n",
    "- <b>Document</b>\n",
    "    - Contains multiple words and strung together therefore producing meaning\n",
    "- <b>Term</b>\n",
    "    - A word is a term\n",
    "\n",
    "<img src=\"https://i.imgur.com/F3fSS1v.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;3.</font><font color=\"salmon\"> Text Normalisation </font> </h2></a>\n",
    "\n",
    "Text Normalisation involves several steps and depending on your use case, you can pick either Stemming or Lemmatization.\n",
    "\n",
    "<img src=\"https://i.imgur.com/NuOTXxL.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;4.</font><font color=\"salmon\"> Simple Cleaning </font> </h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'><h3>4.1 Lowercase</h3></a>\n",
    "\n",
    "Make all characters within a string a lower case with `.lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hackwagon\n"
     ]
    }
   ],
   "source": [
    "name = \"HACKWAGON\"\n",
    "print(name.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'><h3>4.2 Strip Spaces</h3></a>\n",
    "\n",
    "Remove extra spaces within the string by using the `.strip()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HACKWAGON\n"
     ]
    }
   ],
   "source": [
    "name = \"      HACKWAGON     \"\n",
    "print(name.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'><h3>4.3 Regex Cleaning</h3></a>\n",
    "\n",
    "Use the regular expression library to remove unwanted characters. To learn more about the regular expression library, click [here](https://regexone.com/)\n",
    "\n",
    "<img src=\"https://i.imgur.com/bJJ9MBD.png\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hackwagon'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "name = \"(Hackwagon)\"\n",
    "name = re.sub(\"[.®'&$’\\\"\\-()]\", \"\", name)\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='P1'><h2> <img src=\"https://cdn.shopify.com/s/files/1/1200/7374/products/book_aec28e76-52ec-44ab-bc01-41df1279c89f_550x825.png?v=1473897430\" width=25 align=\"left\"> <font color=\"darkorange\"> &nbsp; Practice I </font><font color=\"skyblue\"> * </font></h2></a>\n",
    "\n",
    "### Songs-100.csv\n",
    "\n",
    "### Read and create DataFrame\n",
    "\n",
    "Using the `songs-100.csv`, create a DataFrame called `songs_df` and preview the DataFrame with `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Shape of You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Despacito - Remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Despacito (Featuring Daddy Yankee)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Something Just Like This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I'm the One</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                name\n",
       "0           0                        Shape of You\n",
       "1           1                   Despacito - Remix\n",
       "2           2  Despacito (Featuring Daddy Yankee)\n",
       "3           3            Something Just Like This\n",
       "4           4                         I'm the One"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "songs_df = pd.read_csv(\"songs-100.csv\")\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do simple cleaning function \n",
    "\n",
    "Using what you've learnt earlier, create a function called `clean_names()` which will clean every song name by:\n",
    "\n",
    "* Removes characters with the pattern `[.®'&$’\\\"\\-()]`\n",
    "* Lowercases the string\n",
    "* Strips extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_names(title):\n",
    "    title = re.sub(\"[.®'&$’\\\"\\-()]\", \"\", title)\n",
    "    title = title.lower()\n",
    "    title = title.strip()\n",
    "    \n",
    "    return title\n",
    "    # return the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cleaning function \n",
    "\n",
    "Apply the clean function on the `names` column and <b>reassign it back to the <code>names</code> column</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>shape of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>despacito  remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>despacito featuring daddy yankee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>something just like this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>im the one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>humble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>it aint me with selena gomez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>unforgettable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>thats what i like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>i dont wanna live forever fifty shades darker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>xo tour llif3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>stay with alessia cara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>mask off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>congratulations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>swalla feat nicki minaj  ty dolla ign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>castle on the hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>rockabye feat sean paul  annemarie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>believer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>mi gente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>thunder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>say you wont let go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>theres nothing holdin me back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>me rehúso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>galway girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>scared to be lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>closer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>symphony feat zara larsson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>sign of the times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>goosebumps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>young dumb  broke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>there for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>cold feat future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>silence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>too good at goodbyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>just hold on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>look what you made me do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>glorious feat skylar grey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>starving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>reggaetón lento bailemos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>side to side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>otra vez feat j balvin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>i like me better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>in the name of love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>cold water feat justin bieber  mø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>all night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>hear me now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>your song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>ahora dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>friends with bloodpop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>bank account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>bad things with camila cabello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>dont let me down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>body like a back road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>now or never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>dusk till dawn  radio edit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                               name\n",
       "0            0                                       shape of you\n",
       "1            1                                   despacito  remix\n",
       "2            2                   despacito featuring daddy yankee\n",
       "3            3                           something just like this\n",
       "4            4                                         im the one\n",
       "5            5                                             humble\n",
       "6            6                       it aint me with selena gomez\n",
       "7            7                                      unforgettable\n",
       "8            8                                  thats what i like\n",
       "9            9  i dont wanna live forever fifty shades darker ...\n",
       "10          10                                      xo tour llif3\n",
       "11          11                                              paris\n",
       "12          12                             stay with alessia cara\n",
       "13          13                                          attention\n",
       "14          14                                           mask off\n",
       "15          15                                    congratulations\n",
       "16          16              swalla feat nicki minaj  ty dolla ign\n",
       "17          17                                 castle on the hill\n",
       "18          18                 rockabye feat sean paul  annemarie\n",
       "19          19                                           believer\n",
       "20          20                                           mi gente\n",
       "21          21                                            thunder\n",
       "22          22                                say you wont let go\n",
       "23          23                      theres nothing holdin me back\n",
       "24          24                                          me rehúso\n",
       "25          25                                             issues\n",
       "26          26                                        galway girl\n",
       "27          27                                scared to be lonely\n",
       "28          28                                             closer\n",
       "29          29                         symphony feat zara larsson\n",
       "..         ...                                                ...\n",
       "70          70                                  sign of the times\n",
       "71          71                                         goosebumps\n",
       "72          72                                  young dumb  broke\n",
       "73          73                                      there for you\n",
       "74          74                                   cold feat future\n",
       "75          75                                            silence\n",
       "76          76                               too good at goodbyes\n",
       "77          77                                       just hold on\n",
       "78          78                           look what you made me do\n",
       "79          79                          glorious feat skylar grey\n",
       "80          80                                           starving\n",
       "81          81                           reggaetón lento bailemos\n",
       "82          82                                               weak\n",
       "83          83                                       side to side\n",
       "84          84                             otra vez feat j balvin\n",
       "85          85                                   i like me better\n",
       "86          86                                in the name of love\n",
       "87          87                  cold water feat justin bieber  mø\n",
       "88          88                                             malibu\n",
       "89          89                                          all night\n",
       "90          90                                        hear me now\n",
       "91          91                                          your song\n",
       "92          92                                         ahora dice\n",
       "93          93                              friends with bloodpop\n",
       "94          94                                       bank account\n",
       "95          95                     bad things with camila cabello\n",
       "96          96                                   dont let me down\n",
       "97          97                              body like a back road\n",
       "98          98                                       now or never\n",
       "99          99                         dusk till dawn  radio edit\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df[\"name\"] = songs_df[\"name\"].apply(clean_names)\n",
    "songs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> End of Practice I </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;5.</font><font color=\"salmon\"> Tokenisation </font> </h2></a>\n",
    "\n",
    "Tokenisation is the process of splitting up each word by spaces into individual tokens. \n",
    "\n",
    "<img src=\"https://i.imgur.com/LSpV9Y1.png\" width=400>\n",
    "\n",
    "To do so, we will use the `nltk` - `word-tokenize()` function to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['shape', 'of', 'you'],\n",
       " ['despacito', 'remix'],\n",
       " ['despacito', 'featuring', 'daddy', 'yankee'],\n",
       " ['something', 'just', 'like', 'this'],\n",
       " ['im', 'the', 'one'],\n",
       " ['humble'],\n",
       " ['it', 'aint', 'me', 'with', 'selena', 'gomez'],\n",
       " ['unforgettable'],\n",
       " ['thats', 'what', 'i', 'like'],\n",
       " ['i',\n",
       "  'dont',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'live',\n",
       "  'forever',\n",
       "  'fifty',\n",
       "  'shades',\n",
       "  'darker',\n",
       "  'from',\n",
       "  'fifty',\n",
       "  'shades',\n",
       "  'darker',\n",
       "  'original',\n",
       "  'motion',\n",
       "  'picture',\n",
       "  'soundtrack'],\n",
       " ['xo', 'tour', 'llif3'],\n",
       " ['paris'],\n",
       " ['stay', 'with', 'alessia', 'cara'],\n",
       " ['attention'],\n",
       " ['mask', 'off'],\n",
       " ['congratulations'],\n",
       " ['swalla', 'feat', 'nicki', 'minaj', 'ty', 'dolla', 'ign'],\n",
       " ['castle', 'on', 'the', 'hill'],\n",
       " ['rockabye', 'feat', 'sean', 'paul', 'annemarie'],\n",
       " ['believer'],\n",
       " ['mi', 'gente'],\n",
       " ['thunder'],\n",
       " ['say', 'you', 'wont', 'let', 'go'],\n",
       " ['theres', 'nothing', 'holdin', 'me', 'back'],\n",
       " ['me', 'rehúso'],\n",
       " ['issues'],\n",
       " ['galway', 'girl'],\n",
       " ['scared', 'to', 'be', 'lonely'],\n",
       " ['closer'],\n",
       " ['symphony', 'feat', 'zara', 'larsson'],\n",
       " ['i', 'feel', 'it', 'coming'],\n",
       " ['starboy'],\n",
       " ['wild', 'thoughts'],\n",
       " ['slide'],\n",
       " ['new', 'rules'],\n",
       " ['18002738255'],\n",
       " ['passionfruit'],\n",
       " ['rockstar'],\n",
       " ['strip', 'that', 'down'],\n",
       " ['2u', 'feat', 'justin', 'bieber'],\n",
       " ['perfect'],\n",
       " ['call', 'on', 'me', 'ryan', 'riback', 'extended', 'remix'],\n",
       " ['feels'],\n",
       " ['mama'],\n",
       " ['felices', 'los', '4'],\n",
       " ['ispy', 'feat', 'lil', 'yachty'],\n",
       " ['location'],\n",
       " ['chantaje'],\n",
       " ['bad', 'and', 'boujee', 'feat', 'lil', 'uzi', 'vert'],\n",
       " ['havana'],\n",
       " ['solo', 'dance'],\n",
       " ['fake', 'love'],\n",
       " ['let', 'me', 'love', 'you'],\n",
       " ['more', 'than', 'you', 'know'],\n",
       " ['one', 'dance'],\n",
       " ['subeme', 'la', 'radio'],\n",
       " ['pretty', 'girl', 'cheat', 'codes', 'x', 'cade', 'remix'],\n",
       " ['sorry', 'not', 'sorry'],\n",
       " ['redbone'],\n",
       " ['24k', 'magic'],\n",
       " ['dna'],\n",
       " ['el', 'amante'],\n",
       " ['you', 'dont', 'know', 'me', 'radio', 'edit'],\n",
       " ['chained', 'to', 'the', 'rhythm'],\n",
       " ['no', 'promises', 'feat', 'demi', 'lovato'],\n",
       " ['dont', 'wan', 'na', 'know', 'feat', 'kendrick', 'lamar'],\n",
       " ['how', 'far', 'ill', 'go', 'from', 'moana'],\n",
       " ['slow', 'hands'],\n",
       " ['escápate', 'conmigo'],\n",
       " ['bounce', 'back'],\n",
       " ['sign', 'of', 'the', 'times'],\n",
       " ['goosebumps'],\n",
       " ['young', 'dumb', 'broke'],\n",
       " ['there', 'for', 'you'],\n",
       " ['cold', 'feat', 'future'],\n",
       " ['silence'],\n",
       " ['too', 'good', 'at', 'goodbyes'],\n",
       " ['just', 'hold', 'on'],\n",
       " ['look', 'what', 'you', 'made', 'me', 'do'],\n",
       " ['glorious', 'feat', 'skylar', 'grey'],\n",
       " ['starving'],\n",
       " ['reggaetón', 'lento', 'bailemos'],\n",
       " ['weak'],\n",
       " ['side', 'to', 'side'],\n",
       " ['otra', 'vez', 'feat', 'j', 'balvin'],\n",
       " ['i', 'like', 'me', 'better'],\n",
       " ['in', 'the', 'name', 'of', 'love'],\n",
       " ['cold', 'water', 'feat', 'justin', 'bieber', 'mø'],\n",
       " ['malibu'],\n",
       " ['all', 'night'],\n",
       " ['hear', 'me', 'now'],\n",
       " ['your', 'song'],\n",
       " ['ahora', 'dice'],\n",
       " ['friends', 'with', 'bloodpop'],\n",
       " ['bank', 'account'],\n",
       " ['bad', 'things', 'with', 'camila', 'cabello'],\n",
       " ['dont', 'let', 'me', 'down'],\n",
       " ['body', 'like', 'a', 'back', 'road'],\n",
       " ['now', 'or', 'never'],\n",
       " ['dusk', 'till', 'dawn', 'radio', 'edit']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize each title using list comprehension\n",
    "song_titles = songs_df['name'].tolist()\n",
    "tokenized = []\n",
    "\n",
    "for song in song_titles:\n",
    "    tokenized.append(word_tokenize(song)) # < \n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Corpus\n",
    "\n",
    "To do either Lemmatisation (Lemming) or Stemming, we will use the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jonchewyl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jonchewyl/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jonchewyl/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jonchewyl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloading word corpus\n",
    "nltk.download('punkt') # STEMMING\n",
    "nltk.download('averaged_perceptron_tagger') # < POS TAGGING\n",
    "nltk.download('wordnet') # LEMMATISATION\n",
    "nltk.download('stopwords') # STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;6.</font><font color=\"salmon\"> Method 1 - Lemmatisation </font> </h2></a>\n",
    "\n",
    "Lemmatisation is a more accurate but slower version of stemming. It works in two parts:\n",
    "\n",
    "1. Part Of Speech Tagging\n",
    "2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'><h3>6.1 Part-Of-Speech (POS) Tagging</h3></a>\n",
    "\n",
    "POS Tagging basically classifies a word as a \n",
    "\n",
    "* nouns,\n",
    "* verbs,\n",
    "* adjectives,\n",
    "* adverbs,\n",
    "* etc.\n",
    "\n",
    "<img src=\"https://i.imgur.com/GQTUrJk.png\" width=800>\n",
    "\n",
    "It takes a list of tokens and adds a tag to each of them\n",
    "\n",
    "<img src=\"https://i.imgur.com/1hDGlsu.png\" width=\"500\"/>\n",
    "\n",
    "The meaning of these tags can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('consolidating', 'VBG'),\n",
       " ('credit', 'NN'),\n",
       " ('card', 'NN'),\n",
       " ('debt', 'NN'),\n",
       " ('incurred', 'VBN'),\n",
       " ('over', 'IN'),\n",
       " ('three', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('having', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('concrete', 'JJ'),\n",
       " ('end', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('sight', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('more', 'RBR'),\n",
       " ('motivating', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('eagerly', 'RB'),\n",
       " ('striving', 'JJ'),\n",
       " ('towards', 'NNS'),\n",
       " ('becoming', 'VBG'),\n",
       " ('completely', 'RB'),\n",
       " ('debt', 'NN'),\n",
       " ('free', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "# nltk.pos_tag(a list of tokens)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'><h3>6.2 Lemmatisation</h3></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/dcps8NV.png\" width=\"400\"/>\n",
    "\n",
    "Lemmatisation, like stemming converts words to their root form.\n",
    "\n",
    "* **HOWEVER**, unlike stemming it doesn't arbitrarily chop off letters.\n",
    "* Instead, it refers to a vocabulary and converts words into their base form\n",
    "* The tokens need to be tagged (which you just did)\n",
    "\n",
    "```\n",
    "  Raw               Stemming        Lemmatisation\n",
    "  --------------    --------------  --------------\n",
    "  consolidating          consolid       consolidate\n",
    "```\n",
    "\n",
    "From the above example we can see that **consolidating** is converted to **consolidate**\n",
    "\n",
    "#### Lemmatisation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'consolidate'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('consolidating','v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lemmatisation as a Function \n",
    "\n",
    "As the lemmatizer works on only a single word, you will have to apply it to each pair to get a lemmatized sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(pair):\n",
    "    word, tag = pair\n",
    "\n",
    "    try:\n",
    "        return lemmatizer.lemmatize(word, pos=tag[0].lower())\n",
    "    except KeyError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'be',\n",
       " 'consolidate',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'debt',\n",
       " 'incur',\n",
       " 'over',\n",
       " 'three',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'concrete',\n",
       " 'end',\n",
       " 'in',\n",
       " 'sight',\n",
       " 'be',\n",
       " 'more',\n",
       " 'motivating',\n",
       " '.',\n",
       " 'I',\n",
       " 'be',\n",
       " 'eagerly',\n",
       " 'striving',\n",
       " 'towards',\n",
       " 'become',\n",
       " 'completely',\n",
       " 'debt',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "# nltk.pos_tag(a list of tokens)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "lemmatized = []\n",
    "\n",
    "for word in tagged_tokens:\n",
    "    lemmatized.append(lemmatize(word))\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;7.</font><font color=\"salmon\"> Method 2 - Stemming </font> </h2></a>\n",
    "\n",
    "Stemming makes words common by chopping off the ends of a word. \n",
    "\n",
    "A common algorithm is Porter's Algorithm. It will convert all of the following words to **`oper`**\n",
    "\n",
    "<img src=\"https://i.imgur.com/trzAobw.png\" width=\"200\"/>\n",
    "\n",
    "How it works is by applying a set of rules to the word\n",
    "\n",
    "**Step 1a** of the Porter Algorithm\n",
    "```\n",
    "SSES ->  SS        caresses  ->  caress\n",
    "IES  ->  I         ponies  ->  poni\n",
    "SS  ->  SS         caress  ->  caress\n",
    "S  ->              cats  ->  cat\n",
    "```\n",
    "\n",
    "More steps [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Now you can stem any word you want\n",
    "stemmer.stem(word)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'consolid',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'debt',\n",
       " 'incur',\n",
       " 'over',\n",
       " 'three',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'concret',\n",
       " 'end',\n",
       " 'in',\n",
       " 'sight',\n",
       " 'is',\n",
       " 'more',\n",
       " 'motiv',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'eagerli',\n",
       " 'strive',\n",
       " 'toward',\n",
       " 'becom',\n",
       " 'complet',\n",
       " 'debt',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem each word using list comprehension\n",
    "stemmed = []\n",
    "\n",
    "for word in tokens:\n",
    "    stemmed.append(stemmer.stem(word))\n",
    "\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;8.</font><font color=\"salmon\"> Stemming vs. Lemming </font> </h2></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/7CgyWwH.png\" width=500>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <b>Below is a difference between Stemming and Lemmatization</b>\n",
    "\n",
    " ```\n",
    "   Raw               Stemming        Lemmatisation\n",
    "   --------------    --------------  --------------\n",
    "              I                 I                 I\n",
    "             am                am                be\n",
    "  consolidating          consolid       consolidate\n",
    "         credit            credit            credit\n",
    "           card              card              card\n",
    "           debt              debt              debt\n",
    "       incurred             incur             incur\n",
    "           over              over              over\n",
    "          three             three             three\n",
    "          years              year              year\n",
    "            ago               ago               ago\n",
    "            and               and               and\n",
    "         having              have              have\n",
    "              a                 a                 a\n",
    "       concrete           concret          concrete\n",
    "            end               end               end\n",
    "             in                in                in\n",
    "          sight             sight             sight\n",
    "             is                is                be\n",
    "           more              more              more\n",
    "     motivating             motiv        motivating\n",
    "              .                 .                 .\n",
    "              I                 I                 I\n",
    "             am                am                be\n",
    "        eagerly           eagerli           eagerly\n",
    "       striving            strive          striving\n",
    "        towards            toward           towards\n",
    "       becoming             becom            become\n",
    "     completely           complet        completely\n",
    "           debt              debt              debt\n",
    "           free              free              free\n",
    " ```\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;9.</font><font color=\"salmon\"> Stop Word Removal </font> </h2></a>\n",
    "\n",
    "Stop words are words which have little to no meaning on the overall sentence. For example, they are\n",
    "\n",
    "* i\n",
    "* me\n",
    "* my\n",
    "* or\n",
    "* a\n",
    "* an\n",
    "\n",
    "A more complete list can be downloaded from the **`stopwords` corpus**.\n",
    "\n",
    "The **`stopwords`** corpus that you downloaded is basically a list words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To remove the stop words, you just filter out tokens which are in the list of stop words\n",
    "\n",
    "* For a string `text` and a list `l`, you can check whether `l` contains `text` by doing\n",
    "\n",
    "```python\n",
    "\n",
    "    # Example\n",
    "    text = 'a'\n",
    "    ab_list = ['a','b']\n",
    "\n",
    "    text in ab_list # True\n",
    "    'c' in ab_list # False\n",
    "```\n",
    "\n",
    "* Here's how an example of how you can choose only tokens that don't exist in stopwords\n",
    "\n",
    "\n",
    "```python\n",
    "tokens = ['Hi', 'I']\n",
    "\n",
    "no_stop_words = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in STOP_WORDS:\n",
    "            no_stop_words.append(word)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consolidating', 'credit', 'card', 'debt', 'incurred', 'three', 'years', 'ago', 'concrete', 'sight', 'motivating', '.', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens =['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "# Write code below\n",
    "filtered = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word.lower() not in STOP_WORDS:\n",
    "            filtered.append(word.lower())\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='P2'><h2> <img src=\"https://cdn.shopify.com/s/files/1/1200/7374/products/book_aec28e76-52ec-44ab-bc01-41df1279c89f_550x825.png?v=1473897430\" width=25 align=\"left\"> <font color=\"darkorange\"> &nbsp; Practice II </font><font color=\"skyblue\"> * </font></h2></a>\n",
    "\n",
    "## Loans Description \n",
    "\n",
    "Open the `loans-descs-1k.csv` file as a DataFrame called `loans_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'loans-descs-1k.csv' does not exist: b'loans-descs-1k.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0bfc5c11c508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloans_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loans-descs-1k.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloans_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jerms\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jerms\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jerms\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jerms\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jerms\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'loans-descs-1k.csv' does not exist: b'loans-descs-1k.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "loans_df = pd.read_csv(\"loans-descs-1k.csv\")\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cleaning function\n",
    "\n",
    "Create a cleaning function called `clean()` that cleans the text by applying the following transformations to the `desc` column\n",
    "\n",
    "* 3 regexes with the patterns (do it 1 at a time)\n",
    "\n",
    "```\n",
    "Borrower added on \\d+/\\d+/\\d+ >|<br>\n",
    "<[a-z]+/?>\n",
    "[-_,$&!.;%]\n",
    "```\n",
    "\n",
    "* Strip any extra spaces\n",
    "* Lowercases the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean loans table here\n",
    "def clean(description):\n",
    "    description = re.sub(\"Borrower added on \\d+/\\d+/\\d+ >|<br>\", \"\", description) #front part of the sentence\n",
    "    description = re.sub(\"<[a-z]+/?>\", \"\", description) #html codes\n",
    "    description = re.sub(\"[-_,$&!.;%]\", \"\",description) #special characters\n",
    "    description = description.strip()\n",
    "    description = description.lower()\n",
    "    \n",
    "    return description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the cleaning function\n",
    "\n",
    "Using the `clean()` function, apply it to the `desc` column of the DataFrame, then <b>reassigning it back to the same column.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>grade</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296</td>\n",
       "      <td>7337222</td>\n",
       "      <td>8999285</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>D</td>\n",
       "      <td>the wedding of our dreams  my fianceacute and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>844</td>\n",
       "      <td>7365470</td>\n",
       "      <td>9027579</td>\n",
       "      <td>31825.0</td>\n",
       "      <td>C</td>\n",
       "      <td>pay off high credit card balances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>676471</td>\n",
       "      <td>864466</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>G</td>\n",
       "      <td>my husband and i have just purchased a new hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278</td>\n",
       "      <td>612322</td>\n",
       "      <td>785185</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>F</td>\n",
       "      <td>debt refi always pay bills on time  debt refi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>951</td>\n",
       "      <td>7051350</td>\n",
       "      <td>8713086</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i am consolidating credit card debt incurred o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>11636314</td>\n",
       "      <td>13608481</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>B</td>\n",
       "      <td>debt consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>168</td>\n",
       "      <td>12977415</td>\n",
       "      <td>15009606</td>\n",
       "      <td>10850.0</td>\n",
       "      <td>D</td>\n",
       "      <td>consolidate credit card debt used to update ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1016</td>\n",
       "      <td>3677311</td>\n",
       "      <td>4640617</td>\n",
       "      <td>17200.0</td>\n",
       "      <td>A</td>\n",
       "      <td>this loan is for some high interest loans i ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>639</td>\n",
       "      <td>3240639</td>\n",
       "      <td>3983526</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>pay off high interest credit card  pay off hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>884</td>\n",
       "      <td>11975761</td>\n",
       "      <td>13967919</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>debt consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>436</td>\n",
       "      <td>6395368</td>\n",
       "      <td>7927476</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>pay off some debt i have incurred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>424</td>\n",
       "      <td>142734</td>\n",
       "      <td>142718</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i currently own a portion of a web business th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>909</td>\n",
       "      <td>12876639</td>\n",
       "      <td>14898774</td>\n",
       "      <td>18750.0</td>\n",
       "      <td>C</td>\n",
       "      <td>i would like to consolidate my debts into one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>517</td>\n",
       "      <td>485111</td>\n",
       "      <td>617922</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i will be using this money to consolidate the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>224</td>\n",
       "      <td>1006766</td>\n",
       "      <td>1233117</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>D</td>\n",
       "      <td>trying to consolidate all 5 creditcards from m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>650</td>\n",
       "      <td>4275348</td>\n",
       "      <td>5457566</td>\n",
       "      <td>3975.0</td>\n",
       "      <td>E</td>\n",
       "      <td>pay down some credit cards and do some project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>560</td>\n",
       "      <td>6326745</td>\n",
       "      <td>7858514</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>F</td>\n",
       "      <td>want to consolidate some things so i can make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>76</td>\n",
       "      <td>3366459</td>\n",
       "      <td>4218925</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>the loan is to help consolidate a couple small...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>480</td>\n",
       "      <td>1062330</td>\n",
       "      <td>1294396</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>eliminate credit card debt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>777</td>\n",
       "      <td>297158</td>\n",
       "      <td>297155</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>consolidate debt and home improvement projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1004</td>\n",
       "      <td>1395862</td>\n",
       "      <td>1643071</td>\n",
       "      <td>7325.0</td>\n",
       "      <td>B</td>\n",
       "      <td>to consolidate my debts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>216</td>\n",
       "      <td>890209</td>\n",
       "      <td>1106883</td>\n",
       "      <td>13300.0</td>\n",
       "      <td>E</td>\n",
       "      <td>looking to pay off high interest rate cards an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1061</td>\n",
       "      <td>5835480</td>\n",
       "      <td>7297213</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>D</td>\n",
       "      <td>i need to replace my a/c uint at home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>382</td>\n",
       "      <td>8568047</td>\n",
       "      <td>10320001</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>C</td>\n",
       "      <td>to pay down credit cards and a credit account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>86</td>\n",
       "      <td>1396222</td>\n",
       "      <td>1643382</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i wanted to pay off my credit card debt becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1088</td>\n",
       "      <td>12405279</td>\n",
       "      <td>14417435</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>credit card debt consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>420</td>\n",
       "      <td>6300016</td>\n",
       "      <td>1750194</td>\n",
       "      <td>14500.0</td>\n",
       "      <td>B</td>\n",
       "      <td>to help fund my daughters weeding  the money w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>360</td>\n",
       "      <td>7345662</td>\n",
       "      <td>9007790</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>paying off higher interest credit cards to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>65</td>\n",
       "      <td>757612</td>\n",
       "      <td>957674</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>i am requesting this loan in order to cover my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>741</td>\n",
       "      <td>481657</td>\n",
       "      <td>612600</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1111</td>\n",
       "      <td>11745083</td>\n",
       "      <td>13737222</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i would like to pay off credit card debt that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>262</td>\n",
       "      <td>1264855</td>\n",
       "      <td>1508106</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>we are launching an alternative education prog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>877</td>\n",
       "      <td>6808213</td>\n",
       "      <td>8430231</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i am attempting to consolidate my debt and clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>963</td>\n",
       "      <td>11915835</td>\n",
       "      <td>13907976</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>C</td>\n",
       "      <td>debt consolidation  debt consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>879</td>\n",
       "      <td>1857072</td>\n",
       "      <td>2159459</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>A</td>\n",
       "      <td>to consolidate debt as part of a longterm fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>393</td>\n",
       "      <td>7346237</td>\n",
       "      <td>9008366</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i intend to use this to pay off high interest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>1058</td>\n",
       "      <td>6307086</td>\n",
       "      <td>7838793</td>\n",
       "      <td>15350.0</td>\n",
       "      <td>A</td>\n",
       "      <td>happier days are just around the corner when i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>803</td>\n",
       "      <td>1419315</td>\n",
       "      <td>1669852</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>pay off credit cards and lower interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>637</td>\n",
       "      <td>1536618</td>\n",
       "      <td>1801872</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>D</td>\n",
       "      <td>i am paying down my credit cards in order to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>277</td>\n",
       "      <td>1414574</td>\n",
       "      <td>1664541</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>need capital for marketing to announce expansi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>583</td>\n",
       "      <td>1524582</td>\n",
       "      <td>1788495</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>refinancing high interest credit cards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>57</td>\n",
       "      <td>10755258</td>\n",
       "      <td>12647389</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>consolidate high interest loans to a more affo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>445</td>\n",
       "      <td>5174617</td>\n",
       "      <td>6496777</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>this loan is to consolidate credit card debt t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>640</td>\n",
       "      <td>62499</td>\n",
       "      <td>195929</td>\n",
       "      <td>22400.0</td>\n",
       "      <td>D</td>\n",
       "      <td>i'm in a similar position to many people in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>73</td>\n",
       "      <td>6898873</td>\n",
       "      <td>8540798</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>D</td>\n",
       "      <td>my son got into some trouble and i need to hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>405</td>\n",
       "      <td>4465363</td>\n",
       "      <td>5697560</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>this loan is so i can consolidate my higher ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>828</td>\n",
       "      <td>9236738</td>\n",
       "      <td>11068764</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>i have to pay my loan that really high of inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>931</td>\n",
       "      <td>2044729</td>\n",
       "      <td>1614598</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>paying off two high int loans at 29 amp 36 con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>443</td>\n",
       "      <td>5794442</td>\n",
       "      <td>5656795</td>\n",
       "      <td>16950.0</td>\n",
       "      <td>C</td>\n",
       "      <td>pay debt bills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>170</td>\n",
       "      <td>9208236</td>\n",
       "      <td>11030173</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>A</td>\n",
       "      <td>consolidating our credit card bills to finally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>1060</td>\n",
       "      <td>6605335</td>\n",
       "      <td>8187477</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>i have a side business with my brother we have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>651</td>\n",
       "      <td>3218599</td>\n",
       "      <td>3961278</td>\n",
       "      <td>25450.0</td>\n",
       "      <td>E</td>\n",
       "      <td>i am requesting this loan to consolidate my de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>192</td>\n",
       "      <td>2475645</td>\n",
       "      <td>2967875</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>E</td>\n",
       "      <td>trying to consolidate high intrest rate cc so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>596</td>\n",
       "      <td>668342</td>\n",
       "      <td>854529</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>this loan is to pay of my high interest credit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>670</td>\n",
       "      <td>9114810</td>\n",
       "      <td>10946962</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>for other reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>42</td>\n",
       "      <td>5758679</td>\n",
       "      <td>7190895</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>B</td>\n",
       "      <td>my loan is for paying some of my credit card a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>306</td>\n",
       "      <td>1399834</td>\n",
       "      <td>1647590</td>\n",
       "      <td>16950.0</td>\n",
       "      <td>G</td>\n",
       "      <td>debt consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>710</td>\n",
       "      <td>10090590</td>\n",
       "      <td>11942055</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>C</td>\n",
       "      <td>i want to consolidate my outstanding debt so t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>727</td>\n",
       "      <td>2078030</td>\n",
       "      <td>2430408</td>\n",
       "      <td>16425.0</td>\n",
       "      <td>A</td>\n",
       "      <td>bathroom/kitchen renovations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>106</td>\n",
       "      <td>1465978</td>\n",
       "      <td>1721855</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i racked up a fair amount of credit card debt ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        id  member_id  loan_amnt grade  \\\n",
       "0           296   7337222    8999285    10000.0     D   \n",
       "1           844   7365470    9027579    31825.0     C   \n",
       "2           110    676471     864466    20000.0     G   \n",
       "3           278    612322     785185    20000.0     F   \n",
       "4           951   7051350    8713086    13000.0     B   \n",
       "5            14  11636314   13608481     7500.0     B   \n",
       "6           168  12977415   15009606    10850.0     D   \n",
       "7          1016   3677311    4640617    17200.0     A   \n",
       "8           639   3240639    3983526    12000.0     B   \n",
       "9           884  11975761   13967919    15000.0     B   \n",
       "10          436   6395368    7927476     9000.0     B   \n",
       "11          424    142734     142718    25000.0     B   \n",
       "12          909  12876639   14898774    18750.0     C   \n",
       "13          517    485111     617922    20000.0     B   \n",
       "14          224   1006766    1233117    10800.0     D   \n",
       "15          650   4275348    5457566     3975.0     E   \n",
       "16          560   6326745    7858514    18000.0     F   \n",
       "17           76   3366459    4218925     6000.0     B   \n",
       "18          480   1062330    1294396    11000.0     A   \n",
       "19          777    297158     297155    10000.0     C   \n",
       "20         1004   1395862    1643071     7325.0     B   \n",
       "21          216    890209    1106883    13300.0     E   \n",
       "22         1061   5835480    7297213     4000.0     D   \n",
       "23          382   8568047   10320001    11200.0     C   \n",
       "24           86   1396222    1643382     7000.0     B   \n",
       "25         1088  12405279   14417435    12000.0     B   \n",
       "26          420   6300016    1750194    14500.0     B   \n",
       "27          360   7345662    9007790    12000.0     C   \n",
       "28           65    757612     957674    20000.0     A   \n",
       "29          741    481657     612600     9600.0     A   \n",
       "..          ...       ...        ...        ...   ...   \n",
       "970        1111  11745083   13737222     8000.0     B   \n",
       "971         262   1264855    1508106    10000.0     A   \n",
       "972         877   6808213    8430231    10000.0     B   \n",
       "973         963  11915835   13907976    22500.0     C   \n",
       "974         879   1857072    2159459     9800.0     A   \n",
       "975         393   7346237    9008366    30000.0     B   \n",
       "976        1058   6307086    7838793    15350.0     A   \n",
       "977         803   1419315    1669852     6000.0     A   \n",
       "978         637   1536618    1801872    18000.0     D   \n",
       "979         277   1414574    1664541     5000.0     B   \n",
       "980         583   1524582    1788495    20000.0     C   \n",
       "981          57  10755258   12647389    15000.0     A   \n",
       "982         445   5174617    6496777    18000.0     C   \n",
       "983         640     62499     195929    22400.0     D   \n",
       "984          73   6898873    8540798     3000.0     D   \n",
       "985         405   4465363    5697560    25000.0     B   \n",
       "986         828   9236738   11068764     3000.0     C   \n",
       "987         931   2044729    1614598    24000.0     C   \n",
       "988         443   5794442    5656795    16950.0     C   \n",
       "989         170   9208236   11030173    30000.0     A   \n",
       "990        1060   6605335    8187477    35000.0     C   \n",
       "991         651   3218599    3961278    25450.0     E   \n",
       "992         192   2475645    2967875    35000.0     E   \n",
       "993         596    668342     854529    14000.0     B   \n",
       "994         670   9114810   10946962     4000.0     C   \n",
       "995          42   5758679    7190895     2700.0     B   \n",
       "996         306   1399834    1647590    16950.0     G   \n",
       "997         710  10090590   11942055    10000.0     C   \n",
       "998         727   2078030    2430408    16425.0     A   \n",
       "999         106   1465978    1721855     8000.0     B   \n",
       "\n",
       "                                                  desc  \n",
       "0    the wedding of our dreams  my fianceacute and ...  \n",
       "1                    pay off high credit card balances  \n",
       "2    my husband and i have just purchased a new hom...  \n",
       "3        debt refi always pay bills on time  debt refi  \n",
       "4    i am consolidating credit card debt incurred o...  \n",
       "5                                   debt consolidation  \n",
       "6    consolidate credit card debt used to update ho...  \n",
       "7    this loan is for some high interest loans i ma...  \n",
       "8    pay off high interest credit card  pay off hig...  \n",
       "9                                   debt consolidation  \n",
       "10                   pay off some debt i have incurred  \n",
       "11   i currently own a portion of a web business th...  \n",
       "12   i would like to consolidate my debts into one ...  \n",
       "13   i will be using this money to consolidate the ...  \n",
       "14   trying to consolidate all 5 creditcards from m...  \n",
       "15   pay down some credit cards and do some project...  \n",
       "16   want to consolidate some things so i can make ...  \n",
       "17   the loan is to help consolidate a couple small...  \n",
       "18                          eliminate credit card debt  \n",
       "19      consolidate debt and home improvement projects  \n",
       "20                             to consolidate my debts  \n",
       "21   looking to pay off high interest rate cards an...  \n",
       "22               i need to replace my a/c uint at home  \n",
       "23       to pay down credit cards and a credit account  \n",
       "24   i wanted to pay off my credit card debt becaus...  \n",
       "25                      credit card debt consolidation  \n",
       "26   to help fund my daughters weeding  the money w...  \n",
       "27   paying off higher interest credit cards to be ...  \n",
       "28   i am requesting this loan in order to cover my...  \n",
       "29                                                      \n",
       "..                                                 ...  \n",
       "970  i would like to pay off credit card debt that ...  \n",
       "971  we are launching an alternative education prog...  \n",
       "972  i am attempting to consolidate my debt and clo...  \n",
       "973             debt consolidation  debt consolidation  \n",
       "974  to consolidate debt as part of a longterm fina...  \n",
       "975  i intend to use this to pay off high interest ...  \n",
       "976  happier days are just around the corner when i...  \n",
       "977            pay off credit cards and lower interest  \n",
       "978  i am paying down my credit cards in order to i...  \n",
       "979  need capital for marketing to announce expansi...  \n",
       "980             refinancing high interest credit cards  \n",
       "981  consolidate high interest loans to a more affo...  \n",
       "982  this loan is to consolidate credit card debt t...  \n",
       "983  i'm in a similar position to many people in th...  \n",
       "984  my son got into some trouble and i need to hel...  \n",
       "985  this loan is so i can consolidate my higher ra...  \n",
       "986  i have to pay my loan that really high of inte...  \n",
       "987  paying off two high int loans at 29 amp 36 con...  \n",
       "988                                     pay debt bills  \n",
       "989  consolidating our credit card bills to finally...  \n",
       "990  i have a side business with my brother we have...  \n",
       "991  i am requesting this loan to consolidate my de...  \n",
       "992  trying to consolidate high intrest rate cc so ...  \n",
       "993  this loan is to pay of my high interest credit...  \n",
       "994                                   for other reason  \n",
       "995  my loan is for paying some of my credit card a...  \n",
       "996                                 debt consolidation  \n",
       "997  i want to consolidate my outstanding debt so t...  \n",
       "998                       bathroom/kitchen renovations  \n",
       "999  i racked up a fair amount of credit card debt ...  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df[\"desc\"] = loans_df[\"desc\"].apply(clean)\n",
    "loans_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise 1 Description\n",
    "\n",
    "Randomly select the **first** description and apply:\n",
    "\n",
    "1. Tokenization\n",
    "2. Stemming\n",
    "3. Stopword Removal\n",
    "\n",
    "**Expected output:**\n",
    "\n",
    "    ['wed', 'dream', 'fianceacut', 'plan', 'familyfocus', 'wed', 'hometown', 'us', 'larg', 'famili', 'parent', 'retir', \"'re\", 'cover', 'expens', 'wed', 'prove', 'expens', 'anticip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wed', 'dream', 'fianceacut', 'plan', 'familyfocus', 'wed', 'hometown', 'us', 'larg', 'famili', 'parent', 'retir', \"'re\", 'cover', 'expens', 'wed', 'prove', 'expens', 'anticip']\n"
     ]
    }
   ],
   "source": [
    "# Store your desc column here\n",
    "desc = loans_df[\"desc\"][0] # Selection \n",
    "desc_tokens = word_tokenize(desc) # Tokenization \n",
    "stemmed = []\n",
    "for word in desc_tokens:\n",
    "    stemmed.append(stemmer.stem(word))\n",
    "\n",
    "stemmed_no_stopwords = []\n",
    "for word in stemmed: \n",
    "    if word not in STOP_WORDS:\n",
    "        stemmed_no_stopwords.append(word)\n",
    "\n",
    "print(stemmed_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> End of Practice II </h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
